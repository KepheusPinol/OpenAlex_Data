{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# This is a sample Jupyter Notebook\n",
    "\n",
    "Below is an example of a code cell. \n",
    "Put your cursor into the cell and press Shift+Enter to execute it and select the next one, or click 'Run Cell' button.\n",
    "\n",
    "Press Double Shift to search everywhere for classes, files, tool windows, actions, and settings.\n",
    "\n",
    "To learn more about Jupyter Notebooks in PyCharm, see [help](https://www.jetbrains.com/help/pycharm/ipython-notebook-support.html).\n",
    "For an overview of PyCharm, go to Help -> Learn IDE features or refer to [our documentation](https://www.jetbrains.com/help/pycharm/getting-started.html)."
   ],
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-01T05:01:40.549426Z",
     "start_time": "2024-10-01T04:58:21.762591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pyalex\n",
    "import json\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "pyalex.config.email = \"chr.brenscheidt@gmail.com\"\n",
    "\n",
    "from pyalex import Works, Authors, Sources, Institutions, Topics, Publishers, Funders, config\n",
    "\n",
    "from pyalex import config\n",
    "\n",
    "config.max_retries = 0\n",
    "config.retry_backoff_factor = 0.1\n",
    "config.retry_http_codes = [429, 500, 503]\n",
    "\n",
    "pager = Works().filter(primary_topic={\"id\":\"T14345\"}).select([\"id\",\"title\",\"authorships\",\"referenced_works\"])\n",
    "\n",
    "referenced_ids = []\n",
    "all_items = []\n",
    "referenced_works_list = []  # New list to hold all the referenced works\n",
    "for page in chain(pager.paginate(per_page=200, n_max=None)):\n",
    "    for item in page:\n",
    "        all_items.append(item)\n",
    "        referenced_works_list.extend(item.get('referenced_works', []))  # Add elements to the new list\n",
    "              \n",
    "        \n",
    "with open(Path(\"publications.json\"), \"w\") as f:\n",
    "    json.dump(all_items, f)\n",
    "    \n",
    "print(f\"Anzahl der IDs: {len(all_items)}\")\n",
    "print(f\"Anzahl der IDs: {len(all_items)}\")\n",
    "print(f\"Anzahl der Referenced Works: {len(referenced_works_list)}\")  # Optional: Print the count of referenced works\n"
   ],
   "id": "fbc121e30a2defb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der IDs: 33181\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-07T05:13:11.156013Z",
     "start_time": "2024-10-07T05:12:41.332935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pyalex\n",
    "import json\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "\n",
    "pyalex.config.email = \"chr.brenscheidt@gmail.com\"\n",
    "from pyalex import Works, config\n",
    "\n",
    "config.max_retries = 0\n",
    "config.retry_backoff_factor = 0.1\n",
    "config.retry_http_codes = [429, 500, 503]\n",
    "\n",
    "pager = Works().filter(ids={\"openalex\":\"W2053522485\"}).select([\"id\", \"title\", \"authorships\", \"referenced_works\", \"cited_by_count\", \"cited_by_api_url\"])\n",
    "#pager = Works().filter(primary_topic={\"id\": \"T13616\"}).select([\"id\", \"title\", \"authorships\", \"referenced_works\", \"referenced_works_count\",\"cited_by_count\"])\n",
    "\n",
    "referenced_ids = []\n",
    "referencing_ids = []\n",
    "all_items = []\n",
    "referenced_works_list = []\n",
    "referenced_works = []\n",
    "referencing_works_list = []\n",
    "\n",
    "\n",
    "# Erstellung der Liste mit allen Publikationen für die Ausgangskollektion\n",
    "def get_publications(pager, all_items, referenced_works_list):\n",
    "    for page in chain(pager.paginate(per_page=200, n_max=None)):\n",
    "        for item in page:\n",
    "            item['id'] = item['id'].replace(\"https://openalex.org/\", \"\")\n",
    "            all_items.append(item)\n",
    "            for ref_id in item.get('referenced_works', []):\n",
    "                ref_id = ref_id.replace(\"https://openalex.org/\", \"\")\n",
    "                referenced_works_list.append({'id': ref_id}) \n",
    "\n",
    "    with open(Path(\"publications.json\"), \"w\") as f:\n",
    "        json.dump(all_items, f)\n",
    "\n",
    "    print(f\"Anzahl der IDs: {len(all_items)}\")\n",
    "    print(f\"Gesamtanzahl der Referenced Works: {len(referenced_works_list)}\")\n",
    "\n",
    "# Erstellen der Liste mit allen IDs der zitierten Publikationen und einem Counter über die Höufigkeit der Zitationen in unterschiedlichen Publikationen\n",
    "\n",
    "def get_referenced_works(referenced_works_list, referenced_ids, referenced_works, all_items):\n",
    "    for ref_id in referenced_works_list:\n",
    "        for ref in referenced_ids:\n",
    "            if ref_id['id'] == ref['id']:\n",
    "                ref['Anzahl'] += 1\n",
    "                break\n",
    "        else:\n",
    "            referenced_ids.append({'id' : ref_id['id'], 'Anzahl' : 1})\n",
    "    \n",
    "    referenced_ids.sort(key=lambda x: x.get('Anzahl', 0), reverse=True)\n",
    "    \n",
    "    with open(Path(\"referenced_ids.json\"), \"w\") as f:\n",
    "        json.dump(referenced_ids, f)\n",
    "    \n",
    "    print(f\"Unique Referenced Works Count: {len(referenced_ids)}\")\n",
    "    summe_anzahl = sum(item.get('Anzahl', 0) for item in referenced_ids)\n",
    "    print(\"Die Summe der 'Anzahl' ist:\", summe_anzahl)\n",
    "    \n",
    "    #Abruf der Metadaten für die IDs in referenced_ids\n",
    "    for item in referenced_ids:\n",
    "            Pager_referenced = Works().filter(ids={\"openalex\":item['id']}).select([\"id\", \"title\", \"authorships\", \"referenced_works\", \"referenced_works_count\"])\n",
    "            for page in chain(Pager_referenced.paginate(per_page=200, n_max=None)):\n",
    "                for item_ref in page:\n",
    "                    item_ref['id'] = item_ref['id'].replace(\"https://openalex.org/\", \"\")\n",
    "                    referenced_works.append({'id' : item_ref['id'], 'Anzahl' : 1, 'title' : item_ref['title'], 'authorships' : item_ref['authorships'], 'referenced_works_count' : item_ref['referenced_works_count']})\n",
    "                    \n",
    "    with open(Path(\"referenced_publications_unique.json\"), \"w\") as f:\n",
    "        json.dump(referenced_works, f)\n",
    "        \n",
    "    summe_referenced_work_Count = sum(item.get('referenced_works_count', 0) for item in all_items)\n",
    "    print(\"Die Summe der 'referenced_works_count' ist:\", summe_anzahl)\n",
    "\n",
    "# Erstellen der Liste mit allen IDs der zitierenden Publikationen und einem Counter über die Höufigkeit der Zitationen in unterschiedlichen Publikationen, sowie den Metadaten\n",
    "def get_referencing_works(referencing_works_list, referencing_ids, all_items):\n",
    "    for item in all_items:\n",
    "            Pager_referencing = Works().filter(cites=item['id']).select([\"id\", \"title\", \"authorships\", \"cited_by_count\"])\n",
    "            for page in chain(Pager_referencing.paginate(per_page=200, n_max=None)):\n",
    "                for item in page:\n",
    "                    item['id'] = item['id'].replace(\"https://openalex.org/\", \"\")\n",
    "                    referencing_works_list.append(item)\n",
    "    \n",
    "    with open(Path(\"referencing_publications.json\"), \"w\") as f:\n",
    "        json.dump(referencing_works_list, f)\n",
    "    \n",
    "    print(f\"Anzahl der Referencing Works: {len(referencing_works_list)}\")\n",
    "    \n",
    "    for ref_id in referencing_works_list:\n",
    "        for ref in referencing_ids:\n",
    "            if ref_id['id'] == ref['id']:\n",
    "                ref['Anzahl'] += 1\n",
    "                break\n",
    "        else:\n",
    "            referencing_ids.append({'id' : ref_id['id'], 'Anzahl' : 1, 'title' : ref_id['title'], 'authorships' : ref_id['authorships'], 'cited_by_count' : ref_id['cited_by_count']})\n",
    "     \n",
    "    referencing_ids.sort(key=lambda x: x.get('Anzahl', 0), reverse=True)\n",
    "    \n",
    "    with open(Path(\"referencings_ids.json\"), \"w\") as f:\n",
    "        json.dump(referencing_ids, f)\n",
    "     \n",
    "    print(f\"Unique Referencing Works Count: {len(referencing_ids)}\")\n",
    "    summe_anzahl = sum(item.get('Anzahl', 0) for item in referencing_ids)\n",
    "    summe_cited_by_count = sum(item.get('cited_by_count', 0) for item in all_items)\n",
    "    \n",
    "    print(\"Die Summe der 'Anzahl' ist:\", summe_anzahl)\n",
    "    print(\"Die Summe der 'cited_by_count' ist:\", summe_cited_by_count)\n",
    "    \n",
    "get_publications(pager, all_items, referenced_works_list)\n",
    "get_referenced_works(referenced_works_list, referenced_ids, referenced_works, all_items)\n",
    "get_referencing_works(referencing_works_list, referencing_ids, all_items)"
   ],
   "id": "5f393bdf28870fc1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der IDs: 1\n",
      "Gesamtanzahl der Referenced Works: 18\n",
      "Unique Referenced Works Count: 18\n",
      "Die Summe der 'Anzahl' ist: 18\n",
      "Die Summe der 'referenced_works_count' ist: 18\n",
      "Anzahl der Referencing Works: 18\n",
      "Unique Referencing Works Count: 18\n",
      "Die Summe der 'Anzahl' ist: 18\n",
      "Die Summe der 'cited_by_count' ist: 19\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-10T18:57:57.784466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pyalex\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from langdetect import detect\n",
    "from pyalex import Works, config\n",
    "\n",
    "\n",
    "pyalex.config.email = \"chr.brenscheidt@gmail.com\"\n",
    "\n",
    "\n",
    "config.max_retries = 0\n",
    "config.retry_backoff_factor = 0.1\n",
    "config.retry_http_codes = [429, 500, 503]\n",
    "\n",
    "#pager = Works().filter(ids={\"openalex\":\"W2053522485\"}).select([\"id\", \"title\", \"authorships\", \"referenced_works\", \"abstract_inverted_index\", \"cited_by_count\", \"referenced_works_count\",])\n",
    "pager = Works().filter(primary_topic={\"id\": \"T13616\"}).select([\"id\", \"title\", \"authorships\", \"referenced_works\", \"abstract_inverted_index\", \"referenced_works_count\",\"cited_by_count\"])\n",
    "\n",
    "referenced_ids = []\n",
    "referencing_ids = []\n",
    "all_items = []\n",
    "referenced_works_list = []\n",
    "referenced_works = []\n",
    "referencing_works_list = []\n",
    "\n",
    "\n",
    "# Erstellung der Liste mit allen Publikationen für die Ausgangskollektion\n",
    "def get_publications(pager, all_items, referenced_works_list):\n",
    "    for page in chain(pager.paginate(per_page=200, n_max=None)):\n",
    "        for item in page:\n",
    "            referenced_works_id = []\n",
    "            item['id'] = item['id'].replace(\"https://openalex.org/\", \"\")\n",
    "            # Extrahieren der display_name Werte\n",
    "            author_display_names = [authorship[\"author\"][\"display_name\"] for authorship in\n",
    "                        item[\"authorships\"]]\n",
    "            item['authorships'] = author_display_names\n",
    "            item['abstract'] = item[\"abstract\"]           \n",
    "            for ref_id in item.get('referenced_works', []):\n",
    "                ref_id = ref_id.replace(\"https://openalex.org/\", \"\")\n",
    "                referenced_works_list.append({'id': ref_id}) \n",
    "                referenced_works_id.append(ref_id)\n",
    "            all_items.append({'id' : item['id'], 'title' : item['title'], 'authorships' : item['authorships'], 'abstract' : item[\"abstract\"], 'cited_by_count' : item['cited_by_count'], 'referenced_works' : referenced_works_id, 'referenced_works_count' : item['referenced_works_count']})                 \n",
    "\n",
    "    with open(Path(\"publications.json\"), \"w\") as f:\n",
    "        json.dump(all_items, f)\n",
    "\n",
    "    print(f\"Anzahl der IDs: {len(all_items)}\")\n",
    "    print(f\"Gesamtanzahl der Referenced Works: {len(referenced_works_list)}\")\n",
    "\n",
    "# Erstellen der Liste mit allen IDs der zitierten Publikationen und einem Counter über die Höufigkeit der Zitationen in unterschiedlichen Publikationen\n",
    "\n",
    "def get_referenced_works(referenced_works_list, referenced_ids, referenced_works, all_items):\n",
    "    for ref_id in referenced_works_list:\n",
    "        for ref in referenced_ids:\n",
    "            if ref_id['id'] == ref['id']:\n",
    "                ref['Anzahl'] += 1\n",
    "                break\n",
    "        else:\n",
    "            referenced_ids.append({'id' : ref_id['id'], 'Anzahl' : 1})\n",
    "    \n",
    "    referenced_ids.sort(key=lambda x: x.get('Anzahl', 0), reverse=True)\n",
    "    \n",
    "    with open(Path(\"referenced_ids.json\"), \"w\") as f:\n",
    "        json.dump(referenced_ids, f)\n",
    "    \n",
    "    print(f\"Unique Referenced Works Count: {len(referenced_ids)}\")\n",
    "    summe_anzahl = sum(item.get('Anzahl', 0) for item in referenced_ids)\n",
    "    print(\"Die Summe der 'Anzahl' ist:\", summe_anzahl)\n",
    "    \n",
    "    #Abruf der Metadaten für die IDs in referenced_ids\n",
    "    for item in referenced_ids:\n",
    "            Pager_referenced = Works().filter(ids={\"openalex\":item['id']}).select([\"id\", \"title\", \"authorships\", \"referenced_works\", \"referenced_works_count\", \"abstract_inverted_index\"])\n",
    "            for page in chain(Pager_referenced.paginate(per_page=200, n_max=None)):\n",
    "                for item_ref in page:\n",
    "                    item_ref['id'] = item_ref['id'].replace(\"https://openalex.org/\", \"\")\n",
    "                    author_display_names = [authorship[\"author\"][\"display_name\"] for authorship in\n",
    "                        item_ref[\"authorships\"]]\n",
    "                    item_ref['authorships'] = author_display_names\n",
    "                    item_ref['abstract'] = item_ref[\"abstract\"]\n",
    "                    referenced_works.append({'id' : item_ref['id'], 'Anzahl' : 1, 'title' : item_ref['title'], 'authorships' : item_ref['authorships'], 'abstract': item_ref[\"abstract\"], 'referenced_works_count' : item_ref['referenced_works_count'], 'abstract' : item_ref[\"abstract\"]})\n",
    "                    \n",
    "    with open(Path(\"referenced_publications_unique.json\"), \"w\") as f:\n",
    "        json.dump(referenced_works, f)\n",
    "        \n",
    "    summe_referenced_work_Count = sum(item.get('referenced_works_count', 0) for item in all_items)\n",
    "    print(\"Die Summe der 'referenced_works_count' ist:\", summe_referenced_work_Count)\n",
    "\n",
    "# Erstellen der Liste mit allen IDs der zitierenden Publikationen und einem Counter über die Höufigkeit der Zitationen in unterschiedlichen Publikationen, sowie den Metadaten\n",
    "def get_referencing_works(referencing_works_list, referencing_ids, all_items):\n",
    "    for item in all_items:\n",
    "            Pager_referencing = Works().filter(cites=item['id']).select([\"id\", \"title\", \"authorships\", \"cited_by_count\", \"abstract_inverted_index\"])\n",
    "            referencing_works_id = []\n",
    "            for page in chain(Pager_referencing.paginate(per_page=200, n_max=None)):\n",
    "                for item_ref in page:\n",
    "                    item_ref['id'] = item_ref['id'].replace(\"https://openalex.org/\", \"\")\n",
    "                    author_display_names = [authorship[\"author\"][\"display_name\"] for authorship in\n",
    "                    item_ref[\"authorships\"]]\n",
    "                    item_ref['authorships'] = author_display_names\n",
    "                    item_ref['abstract'] = item_ref[\"abstract\"]\n",
    "                    referencing_works_list.append(item_ref)\n",
    "                    referencing_works_id.append(item_ref['id'])\n",
    "            item['referencing works'] = referencing_works_id\n",
    "                                  \n",
    "    \n",
    "    with open(Path(\"referencing_publications.json\"), \"w\") as f:\n",
    "        json.dump(referencing_works_list, f)\n",
    "    \n",
    "    with open(Path(\"publications.json\"), \"w\") as f:\n",
    "        json.dump(all_items, f)\n",
    "    \n",
    "    print(f\"Anzahl der Referencing Works: {len(referencing_works_list)}\")\n",
    "    \n",
    "    for ref_id in referencing_works_list:\n",
    "        for ref in referencing_ids:\n",
    "            if ref_id['id'] == ref['id']:\n",
    "                ref['Anzahl'] += 1\n",
    "                break\n",
    "        else:\n",
    "            referencing_ids.append({'id' : ref_id['id'], 'Anzahl' : 1, 'title' : ref_id['title'], 'authorships' : ref_id['authorships'], 'cited_by_count' : ref_id['cited_by_count'], 'abstract' : ref_id['abstract']})\n",
    "     \n",
    "    referencing_ids.sort(key=lambda x: x.get('Anzahl', 0), reverse=True)\n",
    "    \n",
    "    with open(Path(\"referencings_ids.json\"), \"w\") as f:\n",
    "        json.dump(referencing_ids, f)\n",
    "     \n",
    "    print(f\"Unique Referencing Works Count: {len(referencing_ids)}\")\n",
    "    summe_anzahl = sum(item.get('Anzahl', 0) for item in referencing_ids)\n",
    "    summe_cited_by_count = sum(item.get('cited_by_count', 0) for item in all_items)\n",
    "    \n",
    "    print(\"Die Summe der 'Anzahl' ist:\", summe_anzahl)\n",
    "    print(\"Die Summe der 'cited_by_count' ist:\", summe_cited_by_count)\n",
    "\n",
    "def term_normalisation_referenced(list_publications):\n",
    "    def normalize(text):\n",
    "        if text is None:\n",
    "            text = \"\"\n",
    "        # Konvertieren in Kleinbuchstaben\n",
    "        text = text.lower()\n",
    "        # Entfernen von Sonderzeichen\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    for publication in list_publications:\n",
    "        title = publication.get('title', None)\n",
    "        abstract = publication.get('abstract', None)\n",
    "\n",
    "        # Titel und Abstract normalisieren\n",
    "        normalized_title = normalize(title)\n",
    "        normalized_abstract = normalize(abstract)\n",
    "\n",
    "        # Begriffe kombinieren und zählen\n",
    "        combined_terms = normalized_title + normalized_abstract\n",
    "        publication[\"kombinierte Terme\"] = combined_terms\n",
    "\n",
    "    # Hier wird 'list_publications' als Basisname für die Datei genutzt und '_normalized' angehängt.\n",
    "    with open('referenced_publications_unique.json', \"w\") as f:\n",
    "        json.dump(list_publications, f)\n",
    "    return list_publications\n",
    "\n",
    "def term_normalisation_referencing(list_publications):\n",
    "    def normalize(text):\n",
    "        if text is None:\n",
    "            text = \"\"\n",
    "        # Konvertieren in Kleinbuchstaben\n",
    "        text = text.lower()\n",
    "        # Entfernen von Sonderzeichen\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        return text\n",
    "\n",
    "    for publication in list_publications:\n",
    "        title = publication.get('title', None)\n",
    "        abstract = publication.get('abstract', None)\n",
    "\n",
    "        # Titel und Abstract normalisieren\n",
    "        normalized_title = normalize(title)\n",
    "        normalized_abstract = normalize(abstract)\n",
    "\n",
    "        # Begriffe kombinieren und zählen\n",
    "        combined_terms = normalized_title + normalized_abstract\n",
    "        publication[\"kombinierte Terme\"] = combined_terms\n",
    "\n",
    "    # Hier wird 'list_publications' als Basisname für die Datei genutzt und '_normalized' angehängt.\n",
    "    with open('referencing_publications_unique.json', \"w\") as f:\n",
    "        json.dump(list_publications, f)\n",
    "    return list_publications\n",
    "\n",
    "get_publications(pager, all_items, referenced_works_list)\n",
    "get_referenced_works(referenced_works_list, referenced_ids, referenced_works, all_items)\n",
    "get_referencing_works(referencing_works_list, referencing_ids, all_items)\n",
    "\n",
    "term_normalisation_referenced(referenced_works)\n",
    "term_normalisation_referencing(referencing_ids)\n",
    "\n"
   ],
   "id": "3233c27e7e73a89",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der IDs: 175\n",
      "Gesamtanzahl der Referenced Works: 1466\n",
      "Unique Referenced Works Count: 1157\n",
      "Die Summe der 'Anzahl' ist: 1466\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "eb36a6e67f7e94c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T05:49:03.644585Z",
     "start_time": "2024-10-18T05:48:31.256033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% \n",
    "import pyalex\n",
    "from pyalex import Works, config\n",
    "import json\n",
    "import re\n",
    "import math\n",
    "from itertools import chain\n",
    "from pathlib import Path\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "# Initialisierung mit einem festgelegten Seed für konsistente Ergebnisse in 'langdetect'\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "# Konstante Konfigurationswerte\n",
    "EMAIL = \"chr.brenscheidt@gmail.com\"\n",
    "RETRY_HTTP_CODES = [429, 500, 503]\n",
    "\n",
    "pyalex.config.email = EMAIL\n",
    "\n",
    "config = pyalex.config\n",
    "config.max_retries = 0\n",
    "config.retry_backoff_factor = 0.1\n",
    "config.retry_http_codes = RETRY_HTTP_CODES\n",
    "\n",
    "\n",
    "def setup_pyalex():\n",
    "    \"\"\" Setup pyalex configuration. \"\"\"\n",
    "    pyalex.config.email = EMAIL\n",
    "    config.max_retries = 0\n",
    "    config.retry_backoff_factor = 0.1\n",
    "    config.retry_http_codes = RETRY_HTTP_CODES\n",
    "\n",
    "\n",
    "def get_publications(pager, all_items, referenced_works_list):\n",
    "    for page in chain(pager.paginate(per_page=200, n_max=None)):\n",
    "        for item in page:\n",
    "            item['id'] = item['id'].replace(\"https://openalex.org/\", \"\")\n",
    "            author_display_names = [authorship[\"author\"][\"display_name\"] for authorship in item[\"authorships\"]]\n",
    "            item['authorships'] = author_display_names\n",
    "            item['abstract'] = item[\"abstract\"] or \"\"\n",
    "\n",
    "            referenced_works_id = [\n",
    "                ref_id.replace(\"https://openalex.org/\", \"\")\n",
    "                for ref_id in item.get('referenced_works', [])\n",
    "            ]\n",
    "            referenced_works_list.extend({'id': ref_id} for ref_id in referenced_works_id)\n",
    "\n",
    "            all_items.append({\n",
    "                'id': item['id'], 'title': item['title'], 'authorships': item['authorships'],\n",
    "                'abstract': item[\"abstract\"], 'cited_by_count': item['cited_by_count'],\n",
    "                'referenced_works': referenced_works_id, 'referenced_works_count': item['referenced_works_count']\n",
    "            })\n",
    "\n",
    "    save_to_json(\"publications.json\", all_items)\n",
    "    print(f\"Anzahl der IDs: {len(all_items)}\")\n",
    "    print(f\"Gesamtanzahl der Referenced Works: {len(referenced_works_list)}\")\n",
    "\n",
    "\n",
    "def get_referenced_works(referenced_works_list, referenced_ids, referenced_works, all_items):\n",
    "    for ref_id in referenced_works_list:\n",
    "        for ref in referenced_ids:\n",
    "            if ref_id['id'] == ref['id']:\n",
    "                ref['Anzahl'] += 1\n",
    "                break\n",
    "        else:\n",
    "            referenced_ids.append({'id': ref_id['id'], 'Anzahl': 1})\n",
    "\n",
    "    referenced_ids.sort(key=lambda x: x.get('Anzahl', 0), reverse=True)\n",
    "    save_to_json(\"referenced_ids.json\", referenced_ids)\n",
    "\n",
    "    print(f\"Unique Referenced Works Count: {len(referenced_ids)}\")\n",
    "    summe_anzahl = sum(item.get('Anzahl', 0) for item in referenced_ids)\n",
    "    print(\"Die Summe der 'Anzahl' ist:\", summe_anzahl)\n",
    "\n",
    "    for item in referenced_ids:\n",
    "        Pager_referenced = Works().filter(ids={\"openalex\": item['id']}).select(\n",
    "            [\"id\", \"title\", \"authorships\", \"referenced_works\", \"referenced_works_count\", \"abstract_inverted_index\"])\n",
    "        for page in chain(Pager_referenced.paginate(per_page=200, n_max=None)):\n",
    "            for item_ref in page:\n",
    "                item_ref['id'] = item_ref['id'].replace(\"https://openalex.org/\", \"\")\n",
    "                author_display_names = [authorship[\"author\"][\"display_name\"] for authorship in item_ref[\"authorships\"]]\n",
    "                item_ref['authorships'] = author_display_names\n",
    "                item_ref['abstract'] = item_ref[\"abstract\"] or \"\"\n",
    "                referenced_works.append({\n",
    "                    'id': item_ref['id'], 'Anzahl': 1, 'title': item_ref['title'],\n",
    "                    'authorships': item_ref['authorships'], 'abstract': item_ref['abstract'],\n",
    "                    'referenced_works_count': item_ref['referenced_works_count']\n",
    "                })\n",
    "\n",
    "    save_to_json(\"referenced_publications_unique.json\", referenced_works)\n",
    "\n",
    "    summe_referenced_work_Count = sum(item.get('referenced_works_count', 0) for item in all_items)\n",
    "    print(\"Die Summe der 'referenced_works_count' ist:\", summe_referenced_work_Count)\n",
    "\n",
    "\n",
    "def get_referencing_works(referencing_works_list, referencing_ids, all_items):\n",
    "    for item in all_items:\n",
    "        Pager_referencing = Works().filter(cites=item['id']).select(\n",
    "            [\"id\", \"title\", \"authorships\", \"cited_by_count\", \"abstract_inverted_index\"])\n",
    "        referencing_works_id = []\n",
    "        for page in chain(Pager_referencing.paginate(per_page=200, n_max=None)):\n",
    "            for item_ref in page:\n",
    "                item_ref['id'] = item_ref['id'].replace(\"https://openalex.org/\", \"\")\n",
    "                author_display_names = [authorship[\"author\"][\"display_name\"] for authorship in item_ref[\"authorships\"]]\n",
    "                item_ref['authorships'] = author_display_names\n",
    "                item_ref['abstract'] = item_ref[\"abstract\"] or \"\"\n",
    "                referencing_works_list.append(item_ref)\n",
    "                referencing_works_id.append(item_ref['id'])\n",
    "        item['referencing works'] = referencing_works_id\n",
    "\n",
    "    save_to_json(\"referencing_publications.json\", referencing_works_list)\n",
    "    save_to_json(\"publications.json\", all_items)\n",
    "\n",
    "    print(f\"Anzahl der Referencing Works: {len(referencing_works_list)}\")\n",
    "\n",
    "    for ref_id in referencing_works_list:\n",
    "        for ref in referencing_ids:\n",
    "            if ref_id['id'] == ref['id']:\n",
    "                ref['Anzahl'] += 1\n",
    "                break\n",
    "        else:\n",
    "            referencing_ids.append({\n",
    "                'id': ref_id['id'], 'Anzahl': 1, 'title': ref_id['title'],\n",
    "                'authorships': ref_id['authorships'], 'cited_by_count': ref_id['cited_by_count'],\n",
    "                'abstract': ref_id['abstract']\n",
    "            })\n",
    "\n",
    "    referencing_ids.sort(key=lambda x: x.get('Anzahl', 0), reverse=True)\n",
    "    save_to_json(\"referencing_publications_unique.json\", referencing_ids)\n",
    "\n",
    "    print(f\"Unique Referencing Works Count: {len(referencing_ids)}\")\n",
    "    summe_anzahl = sum(item.get('Anzahl', 0) for item in referencing_ids)\n",
    "    summe_cited_by_count = sum(item.get('cited_by_count', 0) for item in all_items)\n",
    "\n",
    "    print(\"Die Summe der 'Anzahl' ist:\", summe_anzahl)\n",
    "    print(\"Die Summe der 'cited_by_count' ist:\", summe_cited_by_count)\n",
    "\n",
    "\n",
    "def get_stopwords_for_language(language):\n",
    "    \"\"\"\n",
    "    Holt die Stopwords für die gegebene Sprache. Standardmäßig wird Englisch verwendet, falls die Sprache nicht unterstützt wird.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return set(stopwords.words(language))\n",
    "    except OSError:\n",
    "        # Standardmäßig Englisch verwenden, falls die Sprache nicht unterstützt wird\n",
    "        return set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def get_stemmer_for_language(language):\n",
    "    \"\"\"\n",
    "    Gibt den Stemmer für die gegebene Sprache zurück. Standardmäßig wird Englisch verwendet, falls die Sprache nicht unterstützt wird.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return SnowballStemmer(language)\n",
    "    except ValueError:\n",
    "        # Standardmäßig Englisch verwenden, falls die Sprache nicht unterstützt wird\n",
    "        return SnowballStemmer('english')\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "\n",
    "    try:\n",
    "        # Erkennung der Sprache des Textes\n",
    "        language = detect(text)\n",
    "    except LangDetectException:\n",
    "        language = 'english'  # Standardmäßig Englisch verwenden\n",
    "\n",
    "    # Holen der Stopwords für die erkannte Sprache\n",
    "    stop_words = get_stopwords_for_language(language)\n",
    "    stemmer = get_stemmer_for_language(language)\n",
    "\n",
    "\n",
    "    # Text in Kleinbuchstaben umwandeln\n",
    "    text = text.lower()\n",
    "\n",
    "    # Nicht-Wörter entfernen\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Wörter splitten, Stopwörter entfernen und stämmen\n",
    "    words = text.split()\n",
    "    filtered_words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "\n",
    "    # Gefilterte Wörter zu einem String zusammenfügen\n",
    "    normalized_text = ' '.join(filtered_words)\n",
    "\n",
    "    return normalized_text\n",
    "\n",
    "\n",
    "def count_terms(text):\n",
    "    # Text normalisieren\n",
    "    normalized_text = normalize_text(text)\n",
    "    # Texte in Wörter aufteilen\n",
    "    terms = re.findall(r'\\b\\w+\\b', normalized_text)\n",
    "    \n",
    "    # Dictionary zur Speicherung der Zählwerte\n",
    "    term_count = {}\n",
    "\n",
    "    for term in terms:\n",
    "        # Initialisieren des Zählers, falls das Wort noch nicht im Dictionary ist\n",
    "        if term not in term_count:\n",
    "            term_count[term] = 0\n",
    "        # Zählen des Wortes\n",
    "        term_count[term] += 1\n",
    "\n",
    "    return term_count\n",
    "\n",
    "\n",
    "def count_terms_works(term_lists):\n",
    "    \"\"\"\n",
    "    Aggregates terms from a list of term dictionaries, calculating the TF-IDF value for each term.\n",
    "    \n",
    "    Parameters:\n",
    "    term_lists (list): A list of dictionaries where each dictionary contains terms and their counts.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary with terms and their TF-IDF values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove terms already present in 'all_items'[\"kombinierte Terme works\"]\n",
    "    # Assuming 'all_items' is a list of dictionaries containing the key 'kombinierte Terme works'\n",
    "    #existing_terms = set()\n",
    "    #for item in all_items:\n",
    "    #    if 'kombinierte Terme works' in item:\n",
    "    #        existing_terms.update(item['kombinierte Terme works'].keys())\n",
    "\n",
    "    # Filter out the terms already present in 'all_items'[\"kombinierte Terme works\"] from term_lists\n",
    "    #filtered_term_lists = []\n",
    "    #for terms in term_lists:\n",
    "    #    filtered_terms = {term: count for term, count in terms.items() if term not in existing_terms}\n",
    "    #    filtered_term_lists.append(filtered_terms)\n",
    "    \n",
    "    # Calculate number of documents (filtered term lists)\n",
    "    num_documents = len(all_items)\n",
    "    \n",
    "    # Initialize term frequency and document frequency dictionaries\n",
    "    term_frequency = {}\n",
    "    document_frequency = {}\n",
    "\n",
    "    # Calculate term frequency and document frequency\n",
    "    for terms in term_lists:\n",
    "        for term, count in terms.items():\n",
    "            if term not in term_frequency:\n",
    "                term_frequency[term] = count\n",
    "            else:\n",
    "                term_frequency[term] += count\n",
    "\n",
    "            if term not in document_frequency:\n",
    "                document_frequency[term] = 0\n",
    "            else:\n",
    "                document_frequency[term] += 1\n",
    "\n",
    "    # Calculate and store TF-IDF values\n",
    "    tf_idf = {}\n",
    "    for term, tf in term_frequency.items():\n",
    "        df = document_frequency[term]\n",
    "        idf = math.log(num_documents / (1 + df))\n",
    "        tf_idf[term] = tf * idf\n",
    "\n",
    "    # Sort the terms by TF-IDF values in descending order\n",
    "    sorted_tf_idf = sorted(tf_idf.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    return sorted_tf_idf[:10]\n",
    "\n",
    "\n",
    "def term_normalisation(list_publications, filename):\n",
    "    for publication in list_publications:\n",
    "        title = publication.get('title', \"\")\n",
    "        abstract = publication.get('abstract', \"\")\n",
    "\n",
    "        combined_text = f\"{title} {abstract}\"\n",
    "        publication[\"kombinierte Terme Titel und Abstract\"] = count_terms(combined_text)\n",
    "    save_to_json(filename, list_publications)\n",
    "\n",
    "def save_to_json(filename, data):\n",
    "    \"\"\"Hilfsfunktion zum Speichern von Daten in eine JSON-Datei\"\"\"\n",
    "    with open(Path(filename), \"w\") as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "\n",
    "def enrichment_publications_referenced(all_items, referenced_works):\n",
    "    \"\"\"\n",
    "    Enrich the publications by integrating 'kombinierte Terme' from referenced works.\n",
    "    \n",
    "    Parameters:\n",
    "    all_items (list): A list of all publications.\n",
    "    referenced_works (list): A list of dictionaries where each dictionary contains 'id' and 'kombinierte Terme' of a referenced work.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of enriched publications with added 'kombinierte Terme referenced' information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert referenced_works list to a dictionary for quick lookup\n",
    "    referenced_works_dict = {work['id']: work['kombinierte Terme Titel und Abstract'] for work in referenced_works}\n",
    "    combined_terms_works_dict = {work['id']: work['kombinierte Terme Titel und Abstract'] for work in all_items}\n",
    "\n",
    "    # Enrich each item in all_items\n",
    "    for item in all_items:\n",
    "        if 'referenced_works' in item:\n",
    "            combined_terms_referenced = []\n",
    "            for item_ref in item['referenced_works']:\n",
    "                if item_ref in referenced_works_dict:\n",
    "                    if item_ref not in combined_terms_works_dict:\n",
    "                        # Add the 'kombinierte Terme' from the referenced work\n",
    "                        combined_terms_referenced.append(referenced_works_dict[item_ref])\n",
    "            # Aggregate terms and store in the item\n",
    "            item['kombinierte Terme referenced'] = count_terms_works(combined_terms_referenced)\n",
    "            \n",
    "    save_to_json('publications.json', all_items)\n",
    "\n",
    "\n",
    "def enrichment_publications_referencing(all_items, referencing_ids):\n",
    "    \"\"\"\n",
    "    Enrich the publications by integrating 'kombinierte Terme' from referenced works.\n",
    "    \n",
    "    Parameters:\n",
    "    all_items (list): A list of all publications.\n",
    "    referenced_works (list): A list of dictionaries where each dictionary contains 'id' and 'kombinierte Terme' of a referenced work.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of enriched publications with added 'kombinierte Terme referenced' information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert referenced_works list to a dictionary for quick lookup\n",
    "    referencing_works_dict = {work['id']: work['kombinierte Terme Titel und Abstract'] for work in referencing_ids}\n",
    "    combined_terms_works_dict = {work['id']: work['kombinierte Terme Titel und Abstract'] for work in all_items}\n",
    "\n",
    "    # Enrich each item in all_items\n",
    "    for item in all_items:\n",
    "        if 'referencing works' in item:\n",
    "            combined_terms_referencing = []\n",
    "            for item_ref in item['referencing works']:\n",
    "                if item_ref in referencing_works_dict: \n",
    "                    if item_ref not in combined_terms_works_dict:\n",
    "                        # Add the 'kombinierte Terme' from the referenced work\n",
    "                        combined_terms_referencing.append(referencing_works_dict[item_ref])\n",
    "            # Aggregate terms and store in the item\n",
    "            item['kombinierte Terme referencing'] = count_terms_works(combined_terms_referencing)\n",
    "            \n",
    "    save_to_json('publications.json', all_items)\n",
    "\n",
    "\n",
    "# Hauptprogrammfluss\n",
    "#pager = Works().filter(primary_topic={\"id\": \"T13616\"}).select([\"id\", \"title\", \"authorships\", \"referenced_works\", \"abstract_inverted_index\",\"referenced_works_count\", \"cited_by_count\"])\n",
    "\n",
    "pager = Works().filter(ids={\"openalex\":\"W2053522485\"}).select([\"id\", \"title\", \"authorships\", \"referenced_works\", \"abstract_inverted_index\", \"cited_by_count\", \"referenced_works_count\",])\n",
    "\n",
    "referenced_ids = []\n",
    "referencing_ids = []\n",
    "all_items = []\n",
    "referenced_works = []\n",
    "referenced_works_list = []\n",
    "referencing_works_list = []\n",
    "\n",
    "# Beispiel Aufruf der Funktion\n",
    "get_publications(pager, all_items, referenced_works_list)\n",
    "get_referenced_works(referenced_works_list, referenced_ids, referenced_works, all_items)\n",
    "get_referencing_works(referencing_works_list, referencing_ids, all_items)\n",
    "term_normalisation(referenced_works, \"referenced_publications_unique.json\")\n",
    "term_normalisation(referencing_ids, \"referencing_publications_unique.json\")\n",
    "term_normalisation(all_items, \"publications.json\")\n",
    "enrichment_publications_referenced(all_items, referenced_works)\n",
    "enrichment_publications_referencing(all_items, referencing_ids)"
   ],
   "id": "5a0e74e31fe136d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der IDs: 1\n",
      "Gesamtanzahl der Referenced Works: 18\n",
      "Unique Referenced Works Count: 18\n",
      "Die Summe der 'Anzahl' ist: 18\n",
      "Die Summe der 'referenced_works_count' ist: 18\n",
      "Anzahl der Referencing Works: 18\n",
      "Unique Referencing Works Count: 18\n",
      "Die Summe der 'Anzahl' ist: 18\n",
      "Die Summe der 'cited_by_count' ist: 19\n"
     ]
    }
   ],
   "execution_count": 48
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
